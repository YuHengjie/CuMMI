# %%
import pickle
import sys
import re
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from scipy.stats import boxcox
from datetime import datetime 
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import random
from torch.optim.lr_scheduler import ReduceLROnPlateau
import os
from datetime import datetime
from scipy.stats import pearsonr, spearmanr
from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score
import argparse
import json
# 设定设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 设置随机数种子
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)                     
    torch.cuda.manual_seed(seed)                

set_seed(42)

# %%
# ================================
# Step 2: 构建 Dataset
# ================================
class EmbeddingPairDataset(Dataset):
    def __init__(self, df,  text_embed_data, pro_esm_dict, pro_esmfold_dict):
        self.df = df.reset_index(drop=True)
        self.text_embed_data = text_embed_data
        self.pro_esm_dict = pro_esm_dict
        self.pro_esmfold_dict = pro_esmfold_dict

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        
        text_embed = self.text_embed_data[int(row["x_index"])]
        
        # Fetch the protein embedding
        pro_esm_embed = self.pro_esm_dict[row["Accession"]]
        pro_esmfold_embed = self.pro_esmfold_dict[row["Accession"]]
        
        # Affinity category
        rpa = row["Protein corona composition"]
        w = float(row.get("Overall data quality", 1.0))
        
        return torch.tensor(text_embed, dtype=torch.float32), \
               torch.tensor(pro_esm_embed, dtype=torch.float32), \
               torch.tensor(pro_esmfold_embed, dtype=torch.float32), \
               torch.tensor(rpa, dtype=torch.float32), \
               torch.tensor(w, dtype=torch.float32)

# %%
class CrossAttentionClassifierGated(nn.Module):
    def __init__(self, 
                 x_dim,              # 文本 / 纳米材料特征维度
                 pro_seq_dim,        # 蛋白序列特征维度（如 ESM2: 2560）
                 pro_str_dim,        # 蛋白结构特征维度（如 ESMFold: 384）
                 hidden_dim=1024, 
                 dropout=0.3):
        super().__init__()

        # --------- 文本侧：保持不变 ---------
        self.x_mlp = nn.Sequential(
            nn.Linear(x_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # --------- 蛋白侧：seq/struct 各自投影到 hidden_dim，再融合 ---------

        # 序列 & 结构各自投影到 hidden_dim（和 x_mlp 对齐）
        self.proj_seq = nn.Sequential(
            nn.Linear(pro_seq_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        self.proj_str = nn.Sequential(
            nn.Linear(pro_str_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # 维度级残差门控：输入 concat([seq_h, str_h]) ∈ R^{B×2H}
        # 输出 gate g ∈ (0,1)^{B×H}
        self.pro_gate = nn.Sequential(
            nn.Linear(2 * hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Sigmoid()
        )

        # 融合后的蛋白向量，再过一层 BN + ReLU + Dropout
        # 注意：这里没有 Linear，保持“只有一层映射到 hidden_dim”的设定
        self.pro_mlp  = nn.Sequential(
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # --------- 注意力：保持不变 ---------
        self.attn  = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            batch_first=True,
            dropout=dropout/2
        )
        
        # --------- 分类头：保持不变 ---------
        self.classifier  = nn.Sequential(
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim//4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim//4, hidden_dim//16),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim//16, 1)  # 二分类 logits（配合 BCEWithLogitsLoss）
        )
 
        # 初始化参数 
        self._init_weights()
 
    def _init_weights(self):
        # 原本的 x_mlp / pro_mlp / classifier 里的 Linear
        for module in [self.x_mlp, self.classifier]:
            for layer in module:
                if isinstance(layer, nn.Linear):
                    nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')
                    nn.init.constant_(layer.bias, 0.1)

        # 新增的 proj_seq / proj_str / pro_gate 里的 Linear
        for m in [self.proj_seq, self.proj_str] + \
                 [l for l in self.pro_gate if isinstance(l, nn.Linear)]:
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                nn.init.constant_(m.bias, 0.0)

        # 可选：把门控最后一层 bias 初始化为略偏向“序列”
        last_linear = [l for l in self.pro_gate if isinstance(l, nn.Linear)][-1]
        nn.init.constant_(last_linear.bias, -1.0)  # sigmoid(-1)≈0.27，初始更偏 seq

    def forward(self, x_embed, pro_seq_embed, pro_str_embed):
        """
        x_embed       : [B, x_dim]
        pro_seq_embed : [B, pro_seq_dim]  (ESM2 输出等)
        pro_str_embed : [B, pro_str_dim]  (ESMFold 或结构特征)
        """

        # --------- 文本侧：保持不变 ---------
        x_feat = self.x_mlp(x_embed)        # [B, hidden_dim]

        # --------- 蛋白序列+结构的融合 ---------
        # 1) 各自投影到 hidden_dim
        seq_h = self.proj_seq(pro_seq_embed)   # [B, hidden_dim]
        str_h = self.proj_str(pro_str_embed)   # [B, hidden_dim]

        # 2) 维度级残差门控
        #    g ∈ (0,1)^{B×hidden_dim}，每个维度一个 gate
        g = self.pro_gate(torch.cat([seq_h, str_h], dim=-1))      # [B, hidden_dim]

        # 3) 残差形式：从 seq_h 出发，用结构做纠偏
        #    g ≈ 0 → 接近 seq_h； g ≈ 1 → 接近 str_h
        pro_fused = seq_h + g * (str_h - seq_h)                   # [B, hidden_dim]

        # 4) 再过 BN + ReLU + Dropout（和 x_mlp 风格一致）
        pro_feat = self.pro_mlp(pro_fused)                        # [B, hidden_dim]
        
        # --------- 交叉注意力：保持不变 ---------
        x_tok   = x_feat.unsqueeze(1)   # [B, 1, hidden_dim]
        pro_tok = pro_feat.unsqueeze(1) # [B, 1, hidden_dim]
        
        attn_out, _ = self.attn(
            query=x_tok,
            key=pro_tok,
            value=pro_tok,
            need_weights=False
        )
        
        fused_feature = x_tok + attn_out          # [B, 1, hidden_dim]
        fused_feature = fused_feature.squeeze(1)  # [B, hidden_dim]
        
        # --------- 分类输出：保持不变 ---------
        logits = self.classifier(fused_feature).squeeze(-1)   # [B]

        # 返回 logits + gate，方便后面分析 gate 的使用情况
        return logits, g

# %%
def print_model_params_count(model):
    # 遍历模型的每一个子模块
    for name, module in model.named_modules():
        num_params = sum(p.numel() for p in module.parameters() if p.requires_grad)
        if num_params > 0:  # 只打印有参数的模块
            print(f"Layer: {name}, Number of parameters: {num_params}")

# %%
# -------------------------
# 4) 评估工具
# -------------------------
@torch.no_grad()
def _eval_cls(logits_all, y_all, sample_weight=None, thresh=0.5):
    probs = torch.sigmoid(torch.tensor(logits_all)).numpy()
    y_true = torch.tensor(y_all).numpy().astype(int)
    sw = None if sample_weight is None else np.asarray(sample_weight, dtype=float).reshape(-1)

    # 可能出现单类，做 NaN 保护
    if len(np.unique(y_true)) > 1:
        auroc = roc_auc_score(y_true, probs, sample_weight=sw)
        aupr = average_precision_score(y_true, probs, sample_weight=sw)
    else:
        auroc, aupr = np.nan, np.nan

    y_pred = (probs >= thresh).astype(int)
    f1 = f1_score(y_true, y_pred, sample_weight=sw, zero_division=0,)
    acc = accuracy_score(y_true, y_pred, sample_weight=sw)
    return auroc, aupr, f1, acc, probs

def find_best_threshold(probs, y_true, sample_weight=None):
    grid = np.linspace(0.05, 0.95, 19)
    y_true = np.asarray(y_true).astype(int)
    sw = None if sample_weight is None else np.asarray(sample_weight, dtype=float).reshape(-1)

    best_t, best_f1 = 0.5, -1
    for t in grid:
        f1 = f1_score(y_true, (probs >= t).astype(int), sample_weight=sw, zero_division=0)
        if f1 > best_f1:
            best_f1, best_t = f1, t
    return best_t, best_f1

@torch.no_grad()
def evaluate_on_loader(model, loader, threshold=0.5, auto_find=True, sample_weighted=True):
    model.eval()
    logits_all, y_all, w_all = [], [], []
    for text, pro_seq, pro_str, y, w in loader:
        text = text.to(device, non_blocking=True)
        pro_seq = pro_seq.to(device, non_blocking=True)
        pro_str = pro_str.to(device, non_blocking=True)
        y = y.to(device, non_blocking=True)
        w = w.to(device, non_blocking=True)

        logits, _ = model(text, pro_seq, pro_str)
        logits_all.append(logits.detach().cpu())
        y_all.append(y.detach().cpu())
        w_all.append(w.detach().cpu())

    logits_all = torch.cat(logits_all).numpy()
    y_all = torch.cat(y_all).numpy().astype(int)
    w_all = torch.cat(w_all).numpy().astype(float)

    sw = w_all if sample_weighted else None

    # 固定阈值 0.5
    auroc_05, aupr_05, f1_05, acc_05, probs = _eval_cls(logits_all, y_all, sample_weight=sw, thresh=0.5)
    # 新增：固定阈值 0.25
    auroc_025, aupr_025, f1_025, acc_025, _ = _eval_cls(logits_all, y_all, sample_weight=sw, thresh=0.25)

    base_dict = {
        "probs": probs,
        "y_true": y_all,
        "weights": w_all,
        "metrics@0.5":  {"AUROC": auroc_05,  "AUPRC": aupr_05,  "F1": f1_05,  "ACC": acc_05,  "thr": 0.5},
        "metrics@0.25": {"AUROC": auroc_025, "AUPRC": aupr_025, "F1": f1_025, "ACC": acc_025, "thr": 0.25},
    }

    if auto_find:
        best_thr, best_f1 = find_best_threshold(probs, y_all, sample_weight=sw)
        auroc_b, aupr_b, f1_b, acc_b, _ = _eval_cls(logits_all, y_all, sample_weight=sw, thresh=best_thr)
        base_dict["metrics@best_on_test"] = {
            "AUROC": auroc_b, "AUPRC": aupr_b, "F1": f1_b, "ACC": acc_b, "thr": best_thr
        }

    # 使用给定阈值
    auroc_t, aupr_t, f1_t, acc_t, _ = _eval_cls(logits_all, y_all, sample_weight=sw, thresh=threshold)
    base_dict["metrics@thr"] = {"AUROC": auroc_t, "AUPRC": aupr_t, "F1": f1_t, "ACC": acc_t, "thr": threshold}
    return base_dict

def load_threshold_from_history(history_path, load_stage: int):
    """
    从训练保存的 history_*.npy 中，根据 load_stage 选择评估指标与阈值字段，
    找到该指标最优 epoch 对应的阈值，并返回 (thr, best_idx, best_metric)。

    规则：
      - load_stage ∈ {1, 5}: 指标='aupr', 阈值='best_thresh'
      - load_stage ∈ {2, 3, 4}: 指标='avg_aupr', 阈值='val_out_best_t'
    """
    if not os.path.exists(history_path):
        raise FileNotFoundError(f"History file not found: {history_path}")

    hist = np.load(history_path, allow_pickle=True).item()
    if not isinstance(hist, dict):
        raise ValueError(f"History file is not a dict-like object: {history_path}")

    # 根据阶段选择字段
    if load_stage in (1, 5):
        metric_key = "aupr"
        thr_key = "best_thresh"
    elif load_stage in (2, 3, 4):
        metric_key = "avg_aupr"
        thr_key = "val_out_best_t"
    else:
        raise ValueError(f"Unsupported load_stage={load_stage}. Expected one of {{1,2,3,4,5}}.")

    # 读取指标与阈值列表
    metrics = np.array(hist.get(metric_key, []), dtype=float)
    thr_list = np.array(hist.get(thr_key, []), dtype=float)

    # 健壮性检查
    if metrics.size == 0:
        raise ValueError(f"'{metric_key}' is empty or missing in {history_path}. keys={list(hist.keys())}")
    if thr_list.size == 0:
        raise ValueError(f"'{thr_key}' is empty or missing in {history_path}. keys={list(hist.keys())}")
    if len(metrics) != len(thr_list):
        raise ValueError(
            f"Length mismatch in {history_path}: len({metric_key})={len(metrics)} vs len({thr_key})={len(thr_list)}"
        )
    if np.all(np.isnan(metrics)):
        raise ValueError(f"All values in '{metric_key}' are NaN in {history_path}.")

    # 取最优指标对应的索引
    metrics_safe = np.where(np.isnan(metrics), -np.inf, metrics)
    best_idx = int(np.argmax(metrics_safe))

    thr = float(thr_list[best_idx])
    best_metric = float(metrics[best_idx])
    return thr, best_idx, best_metric

# =============== 安全 JSON 转换 ===============
def _to_py(obj):
    """把 numpy / torch 类型安全转成原生 Python 类型，便于 json.dump。"""
    try:
        if isinstance(obj, torch.Tensor):
            obj = obj.detach().cpu().numpy()
    except Exception:
        pass

    if isinstance(obj, dict):
        return {k: _to_py(v) for k, v in obj.items()}
    if isinstance(obj, (list, tuple)):
        return [_to_py(x) for x in obj]
    if isinstance(obj, (np.generic,)):
        return obj.item()
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    return obj

# %%
# 读取第一个文件
with open("../protein_embedding/protein_embeddings_all_esm.pkl", "rb") as f:
    pro_esm_dict = pickle.load(f)
    

# 检查第一个条目的数组维度
first_key = next(iter(pro_esm_dict))  # 获取第一个键
array_shape = pro_esm_dict[first_key].shape
print(f"Array shape (pro_esm_dict) for {first_key}: {array_shape}")


# 读取第一个文件
with open("../protein_embedding/protein_embeddings_all_esmfold.pkl", "rb") as f:
    pro_esmfold_dict = pickle.load(f)
    

# 检查第一个条目的数组维度
first_key = next(iter(pro_esmfold_dict))  # 获取第一个键
array_shape = pro_esmfold_dict[first_key].shape
print(f"Array shape (pro_esmfold_dict) for {first_key}: {array_shape}")

text_embed_data_nonfill = np.load("../text_embedding/text_embeddings_nonfill.npy")  # 替换为文件路径
print(f"Shape for text embedding: {text_embed_data_nonfill.shape}")

# %%
train_df_all = pd.read_csv("../train_basic_10/data/basic_plasma_human_high_train.csv",keep_default_na=False, na_values=[''])
train_df_all

# %%
x_dim = text_embed_data_nonfill.shape[1]
pro_esm_dim = next(iter(pro_esm_dict.values())).shape[0]
pro_esmfold_dim = next(iter(pro_esmfold_dict.values())).shape[0]


# --------- 准备测试集与 DataLoader（只构建一次）---------
train_loader = DataLoader(
    EmbeddingPairDataset(train_df_all, text_embed_data_nonfill, pro_esm_dict, pro_esmfold_dict),
    batch_size=4096, shuffle=False, num_workers=8
)

# %%
model = CrossAttentionClassifierGated(x_dim, pro_esm_dim, pro_esmfold_dim).to(device)
model.load_state_dict(torch.load('../train_basic_10/output/stage_12345/saved_model_stage_5.pt', map_location=device))

# %%
thr_history, best_idx, best_metric = load_threshold_from_history('../train_basic_10/output/stage_12345/history_stage_5.npy', 5)
thr_history

# %%
results = evaluate_on_loader(model, train_loader, thr_history)
print("\n========== Internal Test ==========")

m025 = results["metrics@0.25"]
print(f"[@0.25]  AUROC={m025['AUROC']:.4f} | AUPRC={m025['AUPRC']:.4f} | F1={m025['F1']:.4f} | ACC={m025['ACC']:.4f}")

m05 = results["metrics@0.5"]
print(f"[@0.5]   AUROC={m05['AUROC']:.4f} | AUPRC={m05['AUPRC']:.4f} | F1={m05['F1']:.4f} | ACC={m05['ACC']:.4f}")

mt = results["metrics@thr"]
print(f"[@Thr ]  thr={mt['thr']:.2f} | AUROC={mt['AUROC']:.4f} | AUPRC={mt['AUPRC']:.4f} | F1={mt['F1']:.4f} | ACC={mt['ACC']:.4f}")

mb = results["metrics@best_on_test"]
print(f"[@Best]  thr={mb['thr']:.2f} | AUROC={mb['AUROC']:.4f} | AUPRC={mb['AUPRC']:.4f} | F1={mb['F1']:.4f} | ACC={mb['ACC']:.4f}")


# %%
result_df = pd.DataFrame(columns=['Feature', 'AUROC', 'AUPRC', 'F1', 'ACC'])
result_df

# %%
# 创建一个新的包含当前特征和指标的 DataFrame
current_result = pd.DataFrame({
    'Feature': ['Baseline'],
    'AUROC': [results["metrics@thr"]["AUROC"]],
    'AUPRC': [results["metrics@thr"]["AUPRC"]], 
    'F1': [results["metrics@thr"]["F1"]],
    'ACC': [results["metrics@thr"]["ACC"]],
})

# 将当前结果保存到 result_df
result_df = pd.concat([result_df, current_result], ignore_index=True)
result_df

# %%
# 读取 JSON 文件
with open('../model_explain/feature_config.json', 'r', encoding='utf-8') as f:
    config = json.load(f)
config

# %%
feature_columns = train_df_all.columns[1:38]
feature_columns

# %%
folder_path = "../text_embedding_4_explain/nonfill_module"
for group_name, features in config.items():
    # 清理组名，用于文件名安全（去除特殊字符）
    print(f'Processing {group_name}----------')
    cleaned_group_name = ''.join(c if c.isalpha() or c.isdigit() else '_' for c in group_name)
    file_name = f"text_embeddings_{cleaned_group_name}.npy"
    save_path = os.path.join(folder_path, file_name)
    
    x_embed_data_non_fill = np.load(save_path) 
    train_dataset = EmbeddingPairDataset(train_df_all, x_embed_data_non_fill, pro_esm_dict, pro_esmfold_dict)
    train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=False, num_workers=8)
    
    # 使用同一个模型和阈值历史，在当前特征编码上评估
    results = evaluate_on_loader(model, train_loader, thr_history)

    # 创建一个新的包含当前特征和指标的 DataFrame
    current_result = pd.DataFrame({
        'Feature': [group_name],
        'AUROC': [results["metrics@thr"]["AUROC"]],
        'AUPRC': [results["metrics@thr"]["AUPRC"]], 
        'F1': [results["metrics@thr"]["F1"]],
        'ACC': [results["metrics@thr"]["ACC"]],
    })

    # 将当前结果追加到总的 result_df 中
    result_df = pd.concat([result_df, current_result], ignore_index=True)
    
# %%
result_df

# %%
result_df.to_csv('tabular_feature_module.csv', index=False)

# %%