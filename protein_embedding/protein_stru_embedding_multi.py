# %%
import torch
from tqdm import tqdm
import pickle
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, EsmForProteinFolding
import multiprocessing as mp
import pickle # å¯¼å…¥ pickle åº“
import os # å¯¼å…¥ os åº“ï¼Œç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨

# %%
# ä½¿ç”¨ GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ==============================
# æµ‹è¯•æ˜¾å­˜å ç”¨
# ==============================
def test_gpu_memory():
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i} æ˜¾å­˜åˆ†é…æƒ…å†µï¼š")
        print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB")
        print(f"  ä¿ç•™: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB")
        print(f"  æ€»æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")

# test_gpu_memory()

# %%
# ==============================
# åˆ†æ®µå‡½æ•°ï¼ˆå¸¦ overlapï¼‰
# ==============================
max_len = 1024   # æ¯æ®µæœ€å¤§é•¿åº¦
overlap = 64

def chunk_sequence(seq, chunk_size=max_len, overlap=overlap):
    """æŠŠåºåˆ—åˆ‡æˆè‹¥å¹²æ®µï¼Œæ¯æ®µæœ‰ overlap ä¸ªæ°¨åŸºé…¸ä¸å‰ä¸€æ®µé‡å ã€‚"""
    if overlap >= chunk_size:
        raise ValueError("overlap å¿…é¡»å°äº chunk_size")

    step = chunk_size - overlap
    chunks = [seq[i:i + chunk_size] for i in range(0, len(seq), step)]
    return chunks

# %%
# å®šä¹‰å…¨å±€ä¿å­˜è·¯å¾„
EMBEDDING_FILE = "esmfold_protein_embeddings.pkl"

def clean_sequence(seq):
    seq = seq.strip().upper()
    valid_aas = set("ACDEFGHIKLMNPQRSTVWY")
    cleaned = "".join([aa if aa in valid_aas else "X" for aa in seq])
    return cleaned

def compute_protein_embedding_single_gpu(seq, model, tokenizer, chunk_size, overlap, device):
    """å•åºåˆ—åˆ‡ç‰‡å¤„ç†å‡½æ•° (ä¸ä½ åŸä»£ç çš„ compute_protein_embedding ç›¸ä¼¼)"""
    # ... (ä½¿ç”¨ä½ åŸä»£ç ä¸­çš„ compute_protein_embedding é€»è¾‘ï¼Œç¡®ä¿å®ƒåªæ¥å—å•ä¸ª seq)
    # æ­¤å¤„çœç•¥å…·ä½“å®ç°ï¼Œæ²¿ç”¨ä½ åŸå§‹ä»£ç ä¸­å•åºåˆ—çš„å¤„ç†é€»è¾‘ï¼Œæ— éœ€ batching
    seq = clean_sequence(seq)
    chunks = chunk_sequence(seq, chunk_size, overlap)
    all_embeddings = []

    with torch.no_grad():
        for chunk in chunks:
            tokenized_input = tokenizer([chunk], return_tensors="pt", 
                                        add_special_tokens=False,padding=True,)["input_ids"].to(device)
            # ç›´æ¥è°ƒç”¨æ¨¡å‹ï¼Œå› ä¸ºæ²¡æœ‰ DataParallel
            output = model(tokenized_input) 
            # ç¡®ä¿ output["states"] çš„ shape æ˜¯ [1, L, 384] æˆ–ç±»ä¼¼
            last_layer = output["states"][-1, 0] 
            chunk_emb = last_layer.mean(dim=0)
            all_embeddings.append(chunk_emb.cpu().to(torch.float32))
            
            # æ¸…ç†æ˜¾å­˜
            del tokenized_input, output, last_layer
            torch.cuda.empty_cache()

    all_embeddings = torch.stack(all_embeddings)
    protein_embedding = all_embeddings.mean(dim=0)
    return protein_embedding.cpu().numpy()

def gpu_worker(rank, df_subset, model_path, tokenizer_path, chunk_size, overlap, result_queue):
    """æ¯ä¸ª GPU è¿›ç¨‹æ‰§è¡Œçš„å‡½æ•°"""
    device = torch.device(f"cuda:{rank}")
    print(f"Worker {rank}: Loading model on {device}")
    
    # åŠ è½½æ¨¡å‹å’Œ tokenizer åˆ°å„è‡ªçš„ GPU
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    model = EsmForProteinFolding.from_pretrained(
        model_path,
        dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
    ).to(device).eval()
    
    # æ¨ç†
    for _, row in tqdm(df_subset.iterrows(), total=len(df_subset), desc=f"GPU {rank} Progress"): # å¢åŠ å­è¿›ç¨‹ tqdm
        accession = row["Accession"]
        seq = row["Sequence"]
        
        try:
            # è°ƒç”¨å•åºåˆ—å¤„ç†å‡½æ•°
            emb = compute_protein_embedding_single_gpu(seq, model, tokenizer, chunk_size, overlap, device)
            result_queue.put((accession, emb))
        except Exception as e:
            print(f"Error processing {accession} on GPU {rank}: {e}")
            # å¦‚æœå‡ºé”™ï¼Œä»ç„¶å‘é€ Noneï¼Œä¸»è¿›ç¨‹ä¼šå¿½ç•¥ï¼Œä½†æˆ‘ä»¬åœ¨è¿™é‡Œä»ç„¶è¦ç»§ç»­
            result_queue.put((accession, None)) 
            
    # **ğŸŒŸ å…³é”®ä¿®æ”¹ï¼šå‘é€ç»“æŸä¿¡å· ğŸŒŸ**
    result_queue.put(('SENTINEL', None)) 
            
    # æ˜¾å¼æ¸…ç†
    del model, tokenizer
    torch.cuda.empty_cache()
    print(f"Worker {rank}: Finished.")
    
# ==============================
# ä¸»è¿›ç¨‹ï¼šåè°ƒä¸ä¿å­˜
# ==============================
def run_parallel_inference(df, model_path, tokenizer_path, chunk_size, overlap, num_gpus):

    # Step A: åŠ è½½å·²æœ‰ç»“æœ
    embedding_dict = {}
    if os.path.exists(EMBEDDING_FILE):
        try:
            with open(EMBEDDING_FILE, "rb") as f:
                embedding_dict = pickle.load(f)
            print(f"Loaded checkpoint with {len(embedding_dict)} embeddings.")
        except Exception as e:
            print(f"Error loading checkpoint: {e}. Starting fresh.")
            embedding_dict = {}

    processed_accessions = set(embedding_dict.keys())
    df_unprocessed = df[~df["Accession"].isin(processed_accessions)]

    if len(df_unprocessed) == 0:
        print("All sequences are already encoded.")
        return embedding_dict

    print(f"Total sequences to process: {len(df_unprocessed)}")

    # Step B: åˆ‡åˆ†æ•°æ®
    num_gpus = min(num_gpus, len(df_unprocessed))
    df_splits = np.array_split(df_unprocessed, num_gpus)
    result_queue = mp.Queue()

    # Step C: å¯åŠ¨å­è¿›ç¨‹
    processes = []
    for rank in range(num_gpus):
        p = mp.Process(
            target=gpu_worker,
            args=(rank, df_splits[rank], model_path, tokenizer_path, chunk_size, overlap, result_queue),
        )
        p.start()
        processes.append(p)

    # Step D: ä¸»è¿›ç¨‹æ”¶é›†ç»“æœå¹¶ä¿å­˜
    active_processes = num_gpus
    SAVE_INTERVAL = max(100, num_gpus)
    pbar = tqdm(total=len(df_unprocessed), desc="Overall Progress")

    while active_processes > 0:
        try:
            accession, emb = result_queue.get(timeout=1)

            if accession == "SENTINEL":
                active_processes -= 1
                continue

            if emb is not None:
                embedding_dict[accession] = emb
                pbar.update(1)

                # å®šæœŸä¿å­˜ï¼ˆåŸå­å†™ï¼‰
                if len(embedding_dict) % SAVE_INTERVAL == 0:
                    tmp_file = EMBEDDING_FILE + ".tmp"
                    with open(tmp_file, "wb") as f:
                        pickle.dump(embedding_dict, f)
                    os.replace(tmp_file, EMBEDDING_FILE)
                    pbar.set_postfix({"Saved": len(embedding_dict)})

        except Exception:
            pass  # é˜Ÿåˆ—æš‚æ—¶ä¸ºç©º

    for p in processes:
        p.join()

    # Step E: æœ€ç»ˆä¿å­˜
    tmp_file = EMBEDDING_FILE + ".tmp"
    with open(tmp_file, "wb") as f:
        pickle.dump(embedding_dict, f)
    os.replace(tmp_file, EMBEDDING_FILE)

    pbar.close()
    print(f"âœ… Finished encoding. Total proteins: {len(embedding_dict)} saved to {EMBEDDING_FILE}")
    return embedding_dict


# %%
# =========================================================
# ä¸»æ‰§è¡Œå…¥å£
# =========================================================
if __name__ == '__main__':
    # 1. è®¾ç½®å¯åŠ¨æ–¹æ³• (æ”¾åœ¨è¿™é‡Œï¼Œé˜²æ­¢é‡å¤è®¾ç½®)
    try:
        # force=True ç¡®ä¿è®¾ç½®ç”Ÿæ•ˆ
        mp.set_start_method('spawn', force=True) 
        print("Multiprocessing start method set to 'spawn'.")
    except RuntimeError as e:
        print(f"Could not set start method: {e}")
        
    # 2. å…¨å±€å‚æ•°å’Œæ•°æ®åŠ è½½ (ä»…åœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œä¸€æ¬¡)
    model_path = "/home/yuhengjie/pt_model/esmfold_v1"
    tokenizer_path = model_path
    num_gpus = 8
    max_len = 1024
    overlap = 64
    
    # å‡è®¾ df å·²åŠ è½½
    # âš ï¸ ç¡®ä¿ pd.read_excel ä¹Ÿåœ¨ if __name__ == '__main__': å—å†…
    df = pd.read_excel("protein_seq_20250418.xlsx", index_col=0)

    # 3. è¿è¡Œå¹¶è¡Œæ¨ç† (ä»…åœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œä¸€æ¬¡)
    print(f"Starting parallel inference on {num_gpus} GPUs...")
    embedding_dict = run_parallel_inference(df, model_path, tokenizer_path, max_len, overlap, num_gpus)

    print(f"Total proteins encoded: {len(embedding_dict)}")
    
# %%